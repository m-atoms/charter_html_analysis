{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the soup (load doc)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify file\n",
    "file_path = 'sf_charter_04182024.html'\n",
    "\n",
    "# Load the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### CHOP UP THE SOUP ###\n",
    "########################\n",
    "# remove content sections we don't want for now\n",
    "# hackin and slashing, do not necessarily endorse\n",
    "\n",
    "############################\n",
    "### REMOVE CHARTER INTRO ###\n",
    "############################\n",
    "\n",
    "# Find the starting point div with title=\"Charter\"\n",
    "anchor_div = None\n",
    "anchor_div = soup.find('a', {'title': 'Charter'}).find_parent('div')\n",
    "\n",
    "# Find the highest-level parent <div> for the anchor\n",
    "parent_div = anchor_div\n",
    "while parent_div.name != 'div' or parent_div.find_parent('div'):\n",
    "    parent_div = parent_div.find_parent()\n",
    "\n",
    "# Remove parent div\n",
    "parent_div.decompose()\n",
    "\n",
    "##############################\n",
    "### REMOVE MUNICIPAL CODES ###\n",
    "##############################\n",
    "\n",
    "# Find starting point div with id=\"rid-0-0-0-0-52172\"\n",
    "# Even more hacky because there's no unique title\n",
    "anchor_div = None\n",
    "anchor_div = soup.find('div', {'id': 'rid-0-0-0-52172'})\n",
    "\n",
    "# Find the highest-level parent <div> for the anchor\n",
    "parent_div = anchor_div\n",
    "while parent_div.name != 'div' or parent_div.find_parent('div'):\n",
    "    parent_div = parent_div.find_parent()\n",
    "\n",
    "# Remove parent div\n",
    "parent_div.decompose()\n",
    "\n",
    "######################\n",
    "### REMOVE PREFACE ###\n",
    "######################\n",
    "\n",
    "# Find starting point div with id=\"rid-0-0-0-8\"\n",
    "# Even more hacky because there's no unique title\n",
    "anchor_div = None\n",
    "anchor_div = soup.find('div', {'id': 'rid-0-0-0-8'})\n",
    "\n",
    "# Find the highest-level parent <div> for the anchor\n",
    "parent_div = anchor_div\n",
    "while parent_div.name != 'div' or parent_div.find_parent('div'):\n",
    "    parent_div = parent_div.find_parent()\n",
    "\n",
    "# Remove parent div\n",
    "parent_div.decompose()\n",
    "\n",
    "##########################\n",
    "### REMOVE APPENDICIES ###\n",
    "##########################\n",
    "\n",
    "# Find the starting point div with title 'Charter Appendices'\n",
    "ancchor_div = None\n",
    "anchor_div = soup.find('a', {'title': 'Charter Appendices'}).find_parent('div')\n",
    "\n",
    "# Find the highest-level parent <div> for the anchor\n",
    "parent_div = anchor_div\n",
    "while parent_div.name != 'div' or parent_div.find_parent('div'):\n",
    "    parent_div = parent_div.find_parent()\n",
    "\n",
    "# Remove all subsequent div elements including the starting one\n",
    "element = parent_div\n",
    "while element:\n",
    "    next_element = element.find_next_sibling('div')\n",
    "    element.decompose()\n",
    "    element = next_element\n",
    "\n",
    "#########################\n",
    "### REMOVE ARTICLE 18 ###\n",
    "#########################\n",
    "\n",
    "# Find the starting point div with title=\"Article XVIII\"\n",
    "anchor_div = None\n",
    "anchor_div = soup.find('a', {'title': 'Article XVIII'}).find_parent('div')\n",
    "\n",
    "# Find the highest-level parent <div> for the anchor\n",
    "parent_div = anchor_div\n",
    "while parent_div.name != 'div' or parent_div.find_parent('div'):\n",
    "    parent_div = parent_div.find_parent()\n",
    "\n",
    "# Remove all subsequent div elements including the starting one\n",
    "element = parent_div\n",
    "while element:\n",
    "    next_element = element.find_next_sibling('div')\n",
    "    element.decompose()\n",
    "    element = next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### CLEAN UP THE SOUP ###\n",
    "#########################\n",
    "# Remove html components we don't want\n",
    "\n",
    "# Remove the head tag\n",
    "for element in soup.find_all('head'):\n",
    "    element.decompose()\n",
    "\n",
    "# Remove all elements with 'annotationdrawer' tag\n",
    "for element in soup.find_all('annotationdrawer'):\n",
    "    element.decompose()\n",
    "\n",
    "# Remove all divs with class=\"clearfix\"\n",
    "for element in soup.find_all('div', class_='clearfix'):\n",
    "    element.decompose()\n",
    "\n",
    "# Find and delete the 'style' attributes in any element\n",
    "elements_with_style = soup.find_all(style=True)\n",
    "for element in elements_with_style:\n",
    "    del element['style']\n",
    "\n",
    "### Delete all tables ###\n",
    "# this kills the table of contents for each Article which is what I want\n",
    "# but it also kills 4 tables (16.116, 16.117, 16.118, 16.119) tough shit!\n",
    "\n",
    "# Delete all divs with class=\"xsl-table\"\n",
    "for element in soup.find_all('div', class_='xsl-table'):\n",
    "    element.decompose()\n",
    "\n",
    "# Delete all <scrolltable> tags\n",
    "for table in soup.find_all('scrolltable'):\n",
    "    table.decompose()\n",
    "\n",
    "### Remove links ###\n",
    "# Replace web links with link text\n",
    "for link in soup.find_all('a', class_='Web'):\n",
    "    link.replace_with(link.text)\n",
    "\n",
    "# Replace jump links with link text\n",
    "for link in soup.find_all('link', class_=\"Jump\"):\n",
    "    link.decompose()\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique html class values \n",
    "\n",
    "def load_html_and_find_classes(soup):\n",
    "    # Find all elements that have a 'class' attribute\n",
    "    elements_with_class = soup.find_all(class_=True)\n",
    "\n",
    "    # Extract unique classes from all elements\n",
    "    unique_classes = set()\n",
    "    for element in elements_with_class:\n",
    "        # Adding all classes found in each element to the set (automatically handles uniqueness)\n",
    "        if element['class']:\n",
    "            unique_classes.update(element['class'])\n",
    "\n",
    "    return unique_classes\n",
    "\n",
    "unique_classes = load_html_and_find_classes(soup)\n",
    "print(\"Unique HTML classes found:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique html title element values\n",
    "\n",
    "# Find all elements with a 'title' attribute\n",
    "elements_with_title = soup.find_all(attrs={\"title\": True})\n",
    "\n",
    "# Extract unique title attribute values and do a half ass sort\n",
    "unique_titles = sorted(set(element['title'] for element in elements_with_title))\n",
    "\n",
    "# Count the total number of unique titles\n",
    "total_unique_titles = len(unique_titles)\n",
    "\n",
    "# Print unique title attribute values\n",
    "print(\"Unique 'title' attribute values found:\", unique_titles)\n",
    "print(\"Total number of unique 'title' attribute values:\", total_unique_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted titles\n",
    "# - empty title\n",
    "# - anything with Note\n",
    "\n",
    "# init vars to catch and count empty and Note\n",
    "titles_note = []\n",
    "t_empty_count = 0\n",
    "\n",
    "filtered_unique_titles = set()\n",
    "for title in unique_titles:\n",
    "\n",
    "    # Count and skip titles containing \"Note\" substring\n",
    "    if \"Note\" in title:\n",
    "        titles_note.append(title) \n",
    "        continue\n",
    "\n",
    "    # Count and skip empty titles\n",
    "    if title.strip() == \"\":\n",
    "        t_empty_count += 1\n",
    "        continue\n",
    "\n",
    "    # Add good titles to unique set\n",
    "    filtered_unique_titles.add(title)\n",
    "\n",
    "# Print info\n",
    "print(\"Unique good titles:\", filtered_unique_titles)\n",
    "print(\"Total unique good titles:\", len(filtered_unique_titles))\n",
    "print(\"Total empty titles:\", t_empty_count)\n",
    "print(\"Titles containing 'Note':\", titles_note)\n",
    "print(\"Total titles containing 'Note':\", len(titles_note))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
